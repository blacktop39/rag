#!/usr/bin/env python3
"""
ë¡œì»¬ LLM (Ollama) ì‚¬ìš© RAG ë°ëª¨
"""

import os
import logging
from dotenv import load_dotenv
from src.enhanced_rag_pipeline import EnhancedRAGPipeline
from src.llm_factory import LLMFactory, LLMConfig

logging.basicConfig(level=logging.INFO)
load_dotenv()

def check_ollama_status():
    """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
    print("ğŸ” Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘...")
    
    base_url = "http://localhost:11434"
    is_running = LLMFactory.check_ollama_server(base_url)
    
    if is_running:
        print("âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
        models = LLMFactory.get_available_ollama_models(base_url)
        if models:
            print(f"ğŸ“‹ ì„¤ì¹˜ëœ ëª¨ë¸ ({len(models)}ê°œ):")
            for model in models:
                print(f"   - {model}")
        else:
            print("âš ï¸  ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    else:
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollamaë¥¼ ì‹œì‘í•˜ì„¸ìš”:")
        print("   ollama serve")
        return False
    
    return True

def recommend_models():
    """ì¶”ì²œ ëª¨ë¸ í‘œì‹œ"""
    print("\nğŸ¯ ì¶”ì²œ Ollama ëª¨ë¸:")
    print("=" * 50)
    
    recommendations = LLMConfig.RECOMMENDED_MODELS["ollama"]
    
    for model, info in recommendations.items():
        size = info.get("size", "Unknown")
        speed = info.get("speed", "Unknown")
        context = info.get("context", "Unknown")
        
        print(f"ğŸ“¦ {model}")
        print(f"   í¬ê¸°: {size}, ì†ë„: {speed}, ì»¨í…ìŠ¤íŠ¸: {context}")
        print(f"   ì„¤ì¹˜: ollama pull {model}")
        print()

def interactive_model_setup():
    """ëŒ€í™”í˜• ëª¨ë¸ ì„¤ì •"""
    base_url = "http://localhost:11434"
    available_models = LLMFactory.get_available_ollama_models(base_url)
    
    if not available_models:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        recommend_models()
        return None
    
    print(f"\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    for i, model in enumerate(available_models, 1):
        print(f"   {i}. {model}")
    
    try:
        choice = int(input(f"\nì„ íƒ (1-{len(available_models)}): "))
        if 1 <= choice <= len(available_models):
            selected_model = available_models[choice - 1].split(":")[0]  # íƒœê·¸ ì œê±°
            print(f"âœ… ì„ íƒëœ ëª¨ë¸: {selected_model}")
            return selected_model
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return None
    except ValueError:
        print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return None

def demo_with_local_llm():
    """ë¡œì»¬ LLM ë°ëª¨"""
    print("ğŸ¦™ ë¡œì»¬ LLM (Ollama) RAG ë°ëª¨")
    print("=" * 50)
    
    # Ollama ìƒíƒœ í™•ì¸
    if not check_ollama_status():
        return
    
    # ëª¨ë¸ ì„ íƒ
    selected_model = interactive_model_setup()
    if not selected_model:
        return
    
    # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
    print(f"\nğŸ”¢ ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
    print("   1. OpenAI (API í‚¤ í•„ìš”)")
    print("   2. SentenceTransformer (ë¡œì»¬)")
    
    embedding_choice = input("ì„ íƒ (1-2): ").strip()
    
    if embedding_choice == "1":
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   SentenceTransformerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            embedding_type = "sentence-transformer"
            embedding_model = "all-MiniLM-L6-v2"
        else:
            embedding_type = "openai"
            embedding_model = "text-embedding-3-small"
    else:
        embedding_type = "sentence-transformer"
        embedding_model = "all-MiniLM-L6-v2"
    
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"   LLM: Ollama/{selected_model}")
    print(f"   ì„ë² ë”©: {embedding_type}/{embedding_model}")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    try:
        print(f"\nğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        rag = EnhancedRAGPipeline(
            llm_type="ollama",
            llm_model=selected_model,
            embedding_type=embedding_type,
            embedding_model=embedding_model,
            collection_name="local_llm_docs"
        )
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ë¬¸ì„œ ë¡œë“œ
    medical_file = "data/medical_info.txt"
    if os.path.exists(medical_file):
        print(f"\nğŸ“„ ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")
        result = rag.add_documents([medical_file])
        if result['success']:
            print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {result['total_chunks']}ê°œ ì²­í¬")
        else:
            print(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {result['failed']}")
            return
    else:
        print(f"âŒ ì˜ë£Œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {medical_file}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ê³ í˜ˆì••ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë‹¹ë‡¨ë³‘ì˜ ì£¼ìš” ì¦ìƒì€?", 
        "ê°ê¸° ì˜ˆë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì‘ê¸‰ìƒí™© ì‹œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
    ]
    
    print(f"\nğŸ¤– í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ({len(test_questions)}ê°œ):")
    print("=" * 50)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n[ì§ˆë¬¸ {i}] {question}")
        print("-" * 30)
        
        try:
            result = rag.generate_answer(question, n_results=3)
            
            print(f"ğŸ©º ë‹µë³€: {result['answer'][:200]}...")
            print(f"ğŸ“š ì¶œì²˜: {', '.join(result.get('sources', []))}")
            print(f"ğŸ”§ ì‚¬ìš© ëª¨ë¸: {result.get('llm_info', 'Unknown')}")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
    
    # ëŒ€í™”í˜• ëª¨ë“œ
    print(f"\n" + "=" * 50)
    print("ğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    user_choice = input("'y' ì…ë ¥ ì‹œ ì‹œì‘: ").strip().lower()
    
    if user_choice in ['y', 'yes']:
        print(f"\nğŸ¯ ë¡œì»¬ LLM ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘!")
        print("ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'")
        print("-" * 30)
        
        while True:
            try:
                question = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                    break
                
                if not question:
                    continue
                
                print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
                result = rag.generate_answer(question)
                
                print(f"\nğŸ¦™ ë‹µë³€: {result['answer']}")
                if result.get('sources'):
                    print(f"ğŸ“š ì¶œì²˜: {', '.join(result['sources'])}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        demo_with_local_llm()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤!")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()