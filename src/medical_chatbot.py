import os
import logging
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate

from .rag_pipeline import RAGPipeline
from .medical_validator import (
    MedicalContentValidator, 
    ConflictDetector, 
    create_safety_enhanced_prompt,
    RiskLevel,
    ReliabilityLevel
)

load_dotenv()
logger = logging.getLogger(__name__)

class MedicalChatbot:
    def __init__(self, 
                 collection_name: str = "medical_documents",
                 persist_directory: str = "./medical_chroma_db",
                 model_name: str = "gpt-3.5-turbo"):
        
        self.rag_pipeline = RAGPipeline(
            collection_name=collection_name,
            persist_directory=persist_directory,
            model_name=model_name
        )
        
        # ì˜ë£Œ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.content_validator = MedicalContentValidator()
        self.conflict_detector = ConflictDetector()
        
        self.medical_system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì˜ë£Œ ì •ë³´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì œê³µëœ ì˜ë£Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

âš ï¸ ì¤‘ìš”í•œ ì•ˆì „ ì§€ì¹¨:
1. ê°œì¸ë³„ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”
2. ì‹¬ê°í•œ ì¦ìƒì˜ ê²½ìš° ì¦‰ì‹œ ì˜ë£Œì§„ ìƒë‹´ì„ ê¶Œí•˜ì„¸ìš”
3. ì¼ë°˜ì ì¸ ì˜ë£Œ ì •ë³´ë§Œ ì œê³µí•˜ê³ , ê°œì¸ ë§ì¶¤ ì˜ë£Œ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”
4. ì‘ê¸‰ìƒí™© ì‹œ 119 ì‹ ê³ ë¥¼ ê¶Œí•˜ì„¸ìš”
5. ëª¨ë“  ë‹µë³€ ëì— "ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤"ë¥¼ í¬í•¨í•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{context}

ë‹µë³€ ì‹œ ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- ê´€ë ¨ ì¦ìƒì´ë‚˜ ì£¼ì˜ì‚¬í•­
- ì¼ë°˜ì ì¸ ì˜ˆë°©ë²•ì´ë‚˜ ê´€ë¦¬ë²•
- ì˜ë£Œì§„ ìƒë‹´ì´ í•„ìš”í•œ ê²½ìš°"""

    def load_medical_documents(self, file_paths: List[str]) -> Dict[str, Any]:
        """ì˜ë£Œ ë¬¸ì„œë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ (ê²€ì¦ í¬í•¨)"""
        try:
            # ë¬¸ì„œ ë¡œë“œ ì „ ì•ˆì „ì„± ê²€ì¦
            validation_results = []
            safe_files = []
            
            for file_path in file_paths:
                if os.path.exists(file_path):
                    # íŒŒì¼ ë‚´ìš© ì½ê¸°
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # ë‚´ìš© ê²€ì¦
                        metadata = {"source": file_path, "file_name": os.path.basename(file_path)}
                        validation = self.content_validator.validate_content(content, metadata)
                        validation_results.append({
                            "file": file_path,
                            "validation": validation
                        })
                        
                        if validation.is_safe:
                            safe_files.append(file_path)
                            logger.info(f"ì•ˆì „ ê²€ì¦ í†µê³¼: {file_path}")
                        else:
                            logger.warning(f"ì•ˆì „ ê²€ì¦ ì‹¤íŒ¨: {file_path} - {validation.warnings}")
                    
                    except Exception as e:
                        logger.error(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨ {file_path}: {e}")
            
            # ì•ˆì „í•œ íŒŒì¼ë“¤ë§Œ ë¡œë“œ
            if safe_files:
                result = self.rag_pipeline.add_documents(safe_files)
                result["validation_results"] = validation_results
                result["safe_files"] = safe_files
                result["blocked_files"] = [f for f in file_paths if f not in safe_files]
                logger.info(f"ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(safe_files)}/{len(file_paths)} íŒŒì¼")
            else:
                result = {
                    "success": [],
                    "failed": file_paths,
                    "total_chunks": 0,
                    "validation_results": validation_results,
                    "safe_files": [],
                    "blocked_files": file_paths
                }
                logger.warning("ë¡œë“œ ê°€ëŠ¥í•œ ì•ˆì „í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
            return result
            
        except Exception as e:
            logger.error(f"ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def ask_medical_question(self, question: str, n_results: int = 3) -> Dict[str, Any]:
        """ì˜ë£Œ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (í–¥ìƒëœ ì•ˆì „ ê²€ì¦ í¬í•¨)"""
        try:
            # 1. ì‘ê¸‰ìƒí™© í‚¤ì›Œë“œ ì²´í¬
            emergency_keywords = ['ì‘ê¸‰', 'ìœ„ê¸‰', 'ì‹¬ê°', 'ì˜ì‹ìƒìŒ', 'í˜¸í¡ê³¤ë€', 'ê°€ìŠ´í†µì¦', 'ì‹¬ì¥ë§ˆë¹„']
            if any(keyword in question for keyword in emergency_keywords):
                return {
                    "answer": "ğŸš¨ ì‘ê¸‰ìƒí™©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì¦‰ì‹œ 119ì— ì‹ ê³ í•˜ê±°ë‚˜ ê°€ì¥ ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤ë¡œ ê°€ì‹œê¸° ë°”ëë‹ˆë‹¤. ì´ëŠ” ì˜ë£Œ ì‘ê¸‰ìƒí™©ì¼ ìˆ˜ ìˆì–´ ì¦‰ê°ì ì¸ ì „ë¬¸ì˜ë£Œì§„ì˜ ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    "type": "emergency",
                    "sources": [],
                    "query": question,
                    "safety_level": "critical"
                }
            
            # 2. RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.rag_pipeline.search_documents(question, n_results)
            
            if not search_results:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì˜ë£Œ ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
                    "type": "no_result",
                    "sources": [],
                    "query": question,
                    "safety_level": "safe"
                }
            
            # 3. ê²€ìƒ‰ëœ ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì¦
            combined_content = "\n".join([result['content'] for result in search_results])
            content_validation = self.content_validator.validate_content(
                combined_content, 
                {"source": "rag_search_results"}
            )
            
            # 4. ì¶©ëŒ ê°ì§€
            conflict_info = self.conflict_detector.detect_conflicts(combined_content, question)
            
            # 5. ìœ„í—˜í•œ ì½˜í…ì¸  ê°ì§€ ì‹œ ì¦‰ì‹œ ì•ˆì „ ì‘ë‹µ
            if content_validation.risk_level == RiskLevel.DANGEROUS:
                warning_msg = "; ".join(content_validation.warnings)
                return {
                    "answer": f"""âš ï¸ ê²€ìƒ‰ëœ ì •ë³´ì— ìœ„í—˜í•œ ë‚´ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.
                    
ê°ì§€ëœ ë¬¸ì œ: {warning_msg}

ì•ˆì „ì„ ìœ„í•´ í•´ë‹¹ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
ì •í™•í•œ ì˜ë£Œ ì •ë³´ëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ğŸ¥ ê¶Œì¥ì‚¬í•­: ë³‘ì› ë°©ë¬¸ ë˜ëŠ” ì˜ë£Œ ìƒë‹´ ì „í™”ë¥¼ ì´ìš©í•˜ì„¸ìš”.""",
                    "type": "blocked_dangerous",
                    "sources": [],
                    "query": question,
                    "safety_level": "blocked",
                    "validation_warnings": content_validation.warnings
                }
            
            # 6. ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            enhanced_prompt = create_safety_enhanced_prompt(content_validation, conflict_info)
            
            # 7. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì‹ ë¢°ë„ ì •ë³´ í¬í•¨)
            context_parts = []
            for result in search_results:
                source_info = result['metadata'].get('file_name', 'Unknown')
                reliability_info = ""
                
                # ì‹ ë¢°ë„ ì •ë³´ ì¶”ê°€
                if content_validation.reliability == ReliabilityLevel.LOW:
                    reliability_info = " [ì‹ ë¢°ë„ ë‚®ìŒ]"
                elif content_validation.reliability == ReliabilityLevel.HIGH:
                    reliability_info = " [ì‹ ë¢°í•  ë§Œí•œ ì¶œì²˜]"
                
                context_parts.append(f"[ì¶œì²˜: {source_info}{reliability_info}]\n{result['content']}")
            
            context = "\n\n".join(context_parts)
            
            # 8. LLM ë‹µë³€ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", enhanced_prompt),
                ("human", "ì§ˆë¬¸: {question}")
            ])
            
            chain = prompt | self.rag_pipeline.llm
            
            response = chain.invoke({
                "context": context,
                "question": question
            })
            
            # 9. ì¶”ê°€ ì•ˆì „ ë©”ì‹œì§€ êµ¬ì„±
            safety_messages = ["âš ï¸ ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."]
            
            if content_validation.warnings:
                safety_messages.extend([f"â€¢ {warning}" for warning in content_validation.warnings])
            
            if conflict_info.get("has_conflicts"):
                safety_messages.append("â€¢ ì¼ë°˜ì ì¸ ì˜í•™ ì§€ì‹ê³¼ ë‹¤ë¥¸ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            if content_validation.recommendations:
                safety_messages.extend([f"â€¢ {rec}" for rec in content_validation.recommendations])
            
            final_answer = response.content + "\n\n" + "\n".join(safety_messages)
            
            # 10. ì‘ë‹µ êµ¬ì„±
            sources = list(set([
                result['metadata'].get('file_name', 'Unknown')
                for result in search_results
            ]))
            
            # ì•ˆì „ ë ˆë²¨ ê²°ì •
            safety_level = "safe"
            if content_validation.risk_level == RiskLevel.CAUTION:
                safety_level = "caution"
            elif content_validation.risk_level == RiskLevel.DANGEROUS:
                safety_level = "dangerous"
            
            return {
                "answer": final_answer,
                "type": "medical_info_validated",
                "sources": sources,
                "query": question,
                "search_results": search_results,
                "safety_level": safety_level,
                "validation_result": {
                    "risk_level": content_validation.risk_level.value,
                    "reliability": content_validation.reliability.value,
                    "confidence_score": content_validation.confidence_score,
                    "warnings": content_validation.warnings,
                    "recommendations": content_validation.recommendations
                },
                "conflict_info": conflict_info,
                "enhanced_safety": True
            }
            
        except Exception as e:
            logger.error(f"ì˜ë£Œ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "answer": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜ë£Œì§„ê³¼ ì§ì ‘ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. (ì˜¤ë¥˜: {str(e)})",
                "type": "error",
                "sources": [],
                "query": question,
                "safety_level": "error"
            }

    def get_available_topics(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ë£Œ ì£¼ì œ ëª©ë¡ ë°˜í™˜"""
        topics = [
            "ê³ í˜ˆì•• (Hypertension)",
            "ë‹¹ë‡¨ë³‘ (Diabetes Mellitus)", 
            "ê°ê¸° (Common Cold)",
            "ë…ê° (Influenza)",
            "ê³ ì§€í˜ˆì¦ (Hyperlipidemia)",
            "ê³¨ë‹¤ê³µì¦ (Osteoporosis)",
            "ì‘ê¸‰ìƒí™© ëŒ€ì²˜ë²•"
        ]
        return topics

    def get_stats(self) -> Dict[str, Any]:
        """ì˜ë£Œ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
        stats = self.rag_pipeline.get_stats()
        stats["available_topics"] = self.get_available_topics()
        return stats

    def interactive_chat(self):
        """ëŒ€í™”í˜• ì˜ë£Œ ìƒë‹´ ì±—ë´‡"""
        print("ğŸ¥ ì˜ë£Œ ì •ë³´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        print("=" * 50)
        print("âš ï¸  ì£¼ì˜ì‚¬í•­:")
        print("- ì´ëŠ” ì¼ë°˜ì ì¸ ì˜ë£Œ ì •ë³´ ì œê³µ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤")
        print("- ê°œì¸ë³„ ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì€ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("- ì‹¬ê°í•œ ì¦ìƒì´ ìˆìœ¼ì‹œë©´ ì¦‰ì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì„¸ìš”")
        print("- ì‘ê¸‰ìƒí™© ì‹œ 119ì— ì‹ ê³ í•˜ì„¸ìš”")
        print("=" * 50)
        
        available_topics = self.get_available_topics()
        print(f"\nğŸ“‹ ì´ìš© ê°€ëŠ¥í•œ ì˜ë£Œ ì •ë³´ ì£¼ì œ:")
        for i, topic in enumerate(available_topics, 1):
            print(f"  {i}. {topic}")
        
        print(f"\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'):")
        print("-" * 50)
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ì§ˆë¬¸: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("ğŸ‘‹ ì˜ë£Œ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê±´ê°•í•˜ì„¸ìš”!")
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
                result = self.ask_medical_question(user_input)
                
                print(f"\nğŸ©º ë‹µë³€:")
                print("-" * 30)
                print(result["answer"])
                
                if result["sources"]:
                    print(f"\nğŸ“š ì°¸ê³  ìë£Œ: {', '.join(result['sources'])}")
                
                print(f"\nğŸ“Š ë‹µë³€ ìœ í˜•: {result['type']}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì˜ë£Œ ìƒë‹´ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê±´ê°•í•˜ì„¸ìš”!")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì„ ìƒì„±í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ì˜ë£Œ ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = MedicalChatbot()
    
    # ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ
    medical_file = "data/medical_info.txt"
    if os.path.exists(medical_file):
        print("ğŸ“„ ì˜ë£Œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
        result = chatbot.load_medical_documents([medical_file])
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {result['total_chunks']}ê°œ ë¬¸ì„œ ì²­í¬")
    else:
        print(f"âŒ ì˜ë£Œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {medical_file}")
        return
    
    # ëŒ€í™”í˜• ì±—ë´‡ ì‹œì‘
    chatbot.interactive_chat()

if __name__ == "__main__":
    main()